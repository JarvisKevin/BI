Q1: 在CTR点击率预估中，使用GBDT+LR的原理是什么？
A1: GBDT是基于Gradient Boosting的树模型方法，可以看作是T棵树的加法模型，每一棵树是基于上一棵树的残差来学习；而LR是指逻辑回归，用于二分类，
    但是LR属于简单的线性模型，在此之前，需要做大量的特征工程，因此，GBDT+LR基于stacking的思想，先训练GBDT模型，将特征进行组合，再固定GBDT
    的参数，将各棵树的结果以onehot的方式表达，再拼接，作为LR的输入，接着训练LR，学习各个特征的权重，以得到最终的分类结果。

Q2: Wide&Deep的模型结构是怎样的，为什么能通过具备记忆和泛化能力（memorization and generalization）
A2: Wide&Deep的模型结构是线性模型和多层感知机(MLP)的并联组合。线性模型的输入是人工特征，一般是一阶或者人工指定的交叉特征，需要做特征工程。
    而MLP的输入则是类别特征的embedding和数值特征的组合。由于线性模型比较简单，对于已知的数据能较好地拟合，因此具备记忆能力，而MLP更适合对
    特征的多阶组合进行学习，有能力进行推理，泛化能力强。因此两者属于互补关系。

Q3: 在CTR预估中，使用FM与DNN结合的方式，有哪些结合的方式，代表模型有哪些？
A3: FM与DNN得结合：DeepFM, NFM。
    DeepFM 指的是FM+MLP，其利用FM学习一阶及二阶交叉特征，对于计算量较大得高阶特征，则用MLP来提取，FM和MLP是并行的关系。其相对于Deep&Wide
    最大的区别是FM和MLP的输入是共享的，即免除了Deep&Wide模型中Wide部分要做特征工程的麻烦。  
    NFM同样也结合了MLP和FM，但NFM中FM和MLP是串联关系，一阶特征利用线性模型提取，其他特征则先经过FM，再输入到MLP中。

Q4: GBDT和随机森林都是基于树的算法，它们有什么区别？
A4: GBDT和随机森林(RF)都是基于树的算法，但GBDT的思想是Gradient Boosting，而RF的思想是Bagging。
    Gradient Boosting: 学习T棵树，每棵树的建立都是基于上一颗树损失函数的梯度下降方向，最终将所有的树的结果累加到一起，得到结果。
    Bagging: 每次有放回地抽取N个样本进行训练，得到T棵树，根据这T课树决定最终的结果，分类问题：多数服从少数(voting)；回归问题：取平均(average)

Q5: item流行度在推荐系统中有怎样的应用?
A5: 推荐系统中，对于新用户，我们可以给其推荐流行度高的商品，解决冷启动问题。而对于老用户，我们应该更关注流行度低的商品，注意长尾理论中的长尾部分，
    做个性化推荐。对于不同的应用场景也需要注意区别，例如像唯品会这样的特卖网站，需要打造爆款，其应该基于流行度进行推荐，而对于婚恋网，则应该保持所
    有用户的活跃度，对流行度低的"用户"活跃起来。
