Q1: 奇异值分解SVD的原理是怎样的，都有哪些应用场景？
A1: SVD是一种矩阵分解方法，其将矩阵分解成三个矩阵，其中一个是特征值对角阵。
    对于非方阵A，可以将其转化成A*(A.T)和(A.T)*A两个对称方阵，接着将上述两个方阵进行SVD分解，A*(A.T)=PX(P.T)和(A.T)*A=QY(Q.T)，由于A*(A.T)和(A.T)*A是对称方阵，
    因此XY具有相同的非零特征值。得到A=PZ(Q.T)，Z为所有非零特征值的均值的对角阵。
    
    应用场景：1. 推荐系统中，由于评分矩阵十分稀疏，因此可以利用SVD矩阵分解将评分矩阵分解成User矩阵和Item矩阵(虽然叫SVD分解，但分解出来三个矩阵还是两个矩阵，是可以
                转换的，这里我们只需要两个矩阵)，通过SGD优化算法使得两个矩阵的乘积无限逼近原来的评分矩阵。由于学习出来的两个矩阵是低维稠密的，因此，它们的乘积也会
                是稠密的，从而实现了对未评分区域的预测。   
             2. 利用SVD分解图片矩阵，选取TopK个特征对矩阵进行重构，实现图片的降维压缩。                   

Q2: funkSVD, BiasSVD，SVD++算法之间的区别是怎样的？
A2: funkSVD：利用SGD优化算法来解决矩阵分解问题，得到稠密User矩阵和Item矩阵，最终得到评分预测矩阵；
    BiasSVD： 在funkSVD的基础上，考虑了User和Item整体的偏差；
    SVD++：除了引入偏差的概念，SVD++进一步考虑了隐式反馈这一特征。

Q3: 矩阵分解算法在推荐系统中有哪些应用场景，存在哪些不足？
A3: 应用场景：传统的SVD可以对矩阵进行无损分解，但实际应用中，我们可以取前K个特征进行降维处理。
    不足：矩阵分解仅仅只考虑了User和item两个维度的特征，其实时间效应、点击次数等等维度也有助于推荐系统对Item进行推荐。

Q4:假设一个小说网站，有N部小说，每部小说都有摘要描述。如何针对该网站制定基于内容的推荐系统，即用户看了某部小说后，推荐其他相关的小说。原理和步骤是怎样的
A4: 原理：相似度计算。
      
    步骤：1. 利用jieba或者nltk对所有小说的摘要描述进行分词，并去除停用词；
         2. 利用TfidfVectorizer将文本进行词频计算并向量化，再计算TFIDF，得到每部小说的文本特征
            (这里需要设置ngram_range参数，以考虑上下文的信息)；
         3. 根据用户看过的小说，计算该小说与其他小说的相似度，为其推荐TopK部相关小说
     	
    思考：上述方法虽然可行，但是其属于词袋模型，特征维度取决于词频，因此可以将每一部小说的摘要描述映射到向量中，接着计算这些向量之间的相似度，以实现基于内容的推荐。

Q5: Word2Vec的应用场景有哪些
A5: 1. 在NLP中，Word2Vec通过将词映射到实数域的向量，其可以实现相关词寻找、情感分析等等。其包含两种模式：1. Skip-Gram，给出指定词，预测上下文；2. CBOW，给定上下文，预测指定词。
      
    2. 在推荐系统中，Word2Vec可以用来对复杂的商品描述进行文本到向量的映射(利用低维的向量来代替多维复杂的文本)，利用这些向量作进一步的计算(例如相似度、距离等)。根据词向量的相似度，
       我们可以基于内容向用户推荐相似的TopN个商品。
