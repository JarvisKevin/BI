{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash, MinHashLSHForest\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import jieba.posseg as pseg\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文本\n",
    "with open('./weibos.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#斯科拉里愿意执教国足#上一届如果是里皮从头芾到尾，是很大机会入世界杯的，这一届，没几个能用的，除非大力归化，谁来都没用',\n",
       " ' \\u200b国足输给叙利亚之后，里皮辞职',\n",
       " '谁将成为新主帅，成为广大球迷关注的焦点',\n",
       " '目前舆论方面，倾向于三个人：山东鲁能主帅李霄鹏、武汉卓尔主帅李铁、前广州恒大主帅斯科拉里',\n",
       " ' \\u200b据了解，无论中国足协态度如何，里皮其实在宣布请辞同时已经去意已决',\n",
       " '据了解',\n",
       " '比赛当晚，他的太太西蒙内塔女士及儿子小里皮都在现场看台上观战',\n",
       " '辞职后的里皮没有改变原有的计划——赛后第二天他会从迪拜直接飞回意大利',\n",
       " '这意味着，他本来也没打算与球队管理层或中国足协高层在赛后第一时间内进行有关辞职的对话',\n",
       " '至于辞职以后的善后工作包括合同问题的沟通工作也要待日后双方进一步协商',\n",
       " '让我们回顾一下国足历届外籍教练——里皮，佩兰，卡马乔，杜伊科维奇，阿里·汉，米卢……',\n",
       " '来之前一个比一个有名，来之后一个比一个水，国足踢不好完全是足协的问题，足协不解散重组，你把天王老子请来都不行斯科拉里想执教中国国足',\n",
       " '老头有点意思，凡是里皮干过的地方，他就想试试',\n",
       " '当然，老头也是世界杯冠军教头，万一折在中国这里也没啥丢人的，毕竟里皮也折了嘛',\n",
       " '可以试试',\n",
       " '斯科拉里的水平，还不如里皮',\n",
       " '斯科拉里，看好的不是国足，而是年薪…… \\u200b非常应该辞职',\n",
       " '中国足球，不需要名帅，也不需要外籍教练，因为一点儿毛用也没有',\n",
       " '从施拉普纳到现在，二十余年间，中国足球竟然大踏步的倒退，一点儿也杀不住车，奶奶的，刹车系统坏了',\n",
       " '穿着几百块钱的球衣，几千块钱的球鞋，几万块钱的包，几十万的包机，几百万上千万的年薪赛后，叙利亚主教练在更衣室里给每个队员一个耳光',\n",
       " '主教练说：“赛前老子就再三交代，这一场无论如何都不能赢中国队',\n",
       " '中国援助了我们那么多粮食和美金，如果他们不再援助我们国家，你狗日些要吃土去',\n",
       " '”，球员委屈的说：“七十多分钟了，哪个晓得那个龟儿子往他们家球门踢嘛',\n",
       " '”里皮辞职返回意大利，他的助教马达洛尼随队返回广州',\n",
       " '马达洛尼在接受采访时还原了当时更衣室中的情况：“当时在更衣室，球员们都过来试图说服里皮，让他收回决定，队长郑智尝试阻止他，足协的代表也希望他在考虑一下，我也建议他重新考虑，但无济于事',\n",
       " '”中国足协：接受里皮辞职请求，将深刻反思看了个报道，马达洛尼说：“关于里皮的辞职，我事先也没有被告知，自己也不清楚发生了什么，也许是里皮头脑一热的决定',\n",
       " '足协也可以留下我们，教练组所有成员都愿意留下，我们也可以和其他教练合作',\n",
       " '” \\u200b因为中国队给斯科拉里开足了薪水，给足了他需要的条件',\n",
       " '尤其是最后一句话，看好中国队的潜力，这句话真是太鼓舞人心啦',\n",
       " ' \\u200b带队结果绝不会比里皮更差， \\u200b而且我一定能带国足夺得2022世界杯冠军',\n",
       " '大家支持斯科拉里不',\n",
       " ' \\u200b曾经也以为世界名帅里皮能够带领国足走出迷茫，让国足蒸蒸日上，也让我们国人不再迷茫，吃下一颗定心丸',\n",
       " '现在我才知道，其实最应改变的不是教练，而是我们的国足，我们的训练机制、学习机制、培养机制',\n",
       " '只有真正做好这些，才能使无论哪位名帅接手都能游刃有余地打好比赛',\n",
       " '国足输给叙利亚后，里皮坐不住了，直接辞职了难怪有网友说，爱护生命，远离男足',\n",
       " '男足的水平也就跟南极洲企鹅踢球',\n",
       " '足协主席赠书《红星照我去战斗》送给中国国脚，埃尔克森、里皮懂红色文化吗',\n",
       " '国足昨晚1-2输给叙利亚，赛后主帅里皮宣布辞职']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 断句\n",
    "sentences = re.split('[。！？]', data.replace('\\n', ''))\n",
    "if sentences[len(sentences)-1]=='':\n",
    "    sentences.pop()\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置停用词\n",
    "stopwords=['\\u200b', '，',' ',':','：','“','”','......', '#','#','呀','嘛']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word(sentence, P=2):\n",
    "    '''\n",
    "    sentence: str\n",
    "    N: N_gram\n",
    "    '''\n",
    "    split=''\n",
    "    temp = list(pseg.cut(sentence))      # 分词\n",
    "    for i in temp:\n",
    "        if i.word not in stopwords:\n",
    "            split += i.word                  \n",
    "            split += ' '                     # 将每个词用' '空格隔开\n",
    "\n",
    "    # 获取单个句子的N_gram特征！\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,P))\n",
    "    vectorizer.fit([split])\n",
    "    n_gram_sentence = vectorizer.get_feature_names()\n",
    "\n",
    "    # 由于之前用' '空格隔开了每一个词，因此考虑多元语法时，会参杂' '，因此将其去掉。\n",
    "    n_gram_sentence = list(map(lambda x: x.replace(' ',''), n_gram_sentence))   \n",
    "\n",
    "    return n_gram_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minhash(sentence, num_perm=128):\n",
    "    '''\n",
    "    sentence: str\n",
    "    num_perm: numbers of permutation\n",
    "    '''\n",
    "    m = MinHash(num_perm)\n",
    "    for s in sentence:\n",
    "        m.update(s.encode('utf-8'))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Jarvis\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.813 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "N = 1      # N_gram\n",
    "n_gram_sentences = [split_word(sentence, N) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 128    # permutation\n",
    "minhash_list = [get_minhash(sentence, P) for sentence in n_gram_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat MinHashLSHForest\n",
    "mhlf = MinHashLSHForest(num_perm=P)\n",
    "\n",
    "r = 1 # 指定第r+1个句子进行测试\n",
    "\n",
    "for i,minhash in enumerate(minhash_list):\n",
    "    if i==r: continue                      # 将第r+1句留出来做测试\n",
    "    mhlf.add(i, minhash)\n",
    "\n",
    "mhlf.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原句：\n",
      "  ​国足输给叙利亚之后，里皮辞职\n",
      "\n",
      "n元分词后：\n",
      " ['之后', '叙利亚', '输给', '辞职']\n"
     ]
    }
   ],
   "source": [
    "# 第r+1个句子的基本情况\n",
    "query_sentence = sentences[r]\n",
    "query_n_gram_sentences = n_gram_sentences[r]\n",
    "query_minhash = minhash_list[r]\n",
    "\n",
    "print('原句：\\n', query_sentence)\n",
    "print('\\nn元分词后：\\n', query_n_gram_sentences)\n",
    "#print('\\n对应的minhash: \\n', query_minhash.digest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['id:38', 0.390625, '国足昨晚1-2输给叙利亚，赛后主帅里皮宣布辞职'],\n",
       " ['id:26',\n",
       "  0.03125,\n",
       "  '”中国足协：接受里皮辞职请求，将深刻反思看了个报道，马达洛尼说：“关于里皮的辞职，我事先也没有被告知，自己也不清楚发生了什么，也许是里皮头脑一热的决定']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_list = mhlf.query(query_minhash, k=2)\n",
    "\n",
    "# 保存uery_sentence和MinHashLSHForest找出来的TopK之间的Jaccard\n",
    "jaccard=[]      \n",
    "for i in topk_list:\n",
    "    jaccary_sim = query_minhash.jaccard(minhash_list[i])           \n",
    "    jaccard.append(['id:'+str(i+1), jaccary_sim, sentences[i]])    # 保存格式：[句子id, jaccard相似度, 句子]\n",
    "\n",
    "# 根据jaccard相似度大小进行排序\n",
    "sorted(jaccard, key=lambda x:-x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['id:38', 0.390625, '国足昨晚1-2输给叙利亚，赛后主帅里皮宣布辞职'],\n",
       " ['id:17', 0.0859375, '斯科拉里，看好的不是国足，而是年薪…… \\u200b非常应该辞职'],\n",
       " ['id:24', 0.0625, '”里皮辞职返回意大利，他的助教马达洛尼随队返回广州'],\n",
       " ['id:26',\n",
       "  0.03125,\n",
       "  '”中国足协：接受里皮辞职请求，将深刻反思看了个报道，马达洛尼说：“关于里皮的辞职，我事先也没有被告知，自己也不清楚发生了什么，也许是里皮头脑一热的决定']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_list = mhlf.query(query_minhash, k=4)\n",
    "\n",
    "# 保存uery_sentence和MinHashLSHForest找出来的TopK之间的Jaccard\n",
    "jaccard=[]      \n",
    "for i in topk_list:\n",
    "    jaccary_sim = query_minhash.jaccard(minhash_list[i])           \n",
    "    jaccard.append(['id:'+str(i+1), jaccary_sim, sentences[i]])     # 保存格式：[句子id, jaccard相似度, 句子]\n",
    "\n",
    "# 根据jaccard相似度大小进行排序\n",
    "sorted(jaccard, key=lambda x:-x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从k=4,可以看到id26与id2Jaccard相似度仅为0.03125，而比这大的还有id17和id24，但在k=2时，返回的Top2却是id38和id26。由于这是ANN(近似近邻)，因此，官方文档也给出了2*K的策略，每次想要topK个相似内容，就设置k为K的正整数倍(2*K,3*K,4*K等)，再利用jaccard来排列，找到真正的TopK！！！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
