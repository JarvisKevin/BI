{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "train = pd.read_csv('train.csv')\n",
    "x_train = train.drop('Survived', 1)\n",
    "y_train = train['Survived']\n",
    "\n",
    "x_test = pd.read_csv('test.csv')\n",
    "test_id = x_test['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Pclass       891 non-null    int64  \n",
      " 2   Name         891 non-null    object \n",
      " 3   Sex          891 non-null    object \n",
      " 4   Age          714 non-null    float64\n",
      " 5   SibSp        891 non-null    int64  \n",
      " 6   Parch        891 non-null    int64  \n",
      " 7   Ticket       891 non-null    object \n",
      " 8   Fare         891 non-null    float64\n",
      " 9   Cabin        204 non-null    object \n",
      " 10  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 76.7+ KB\n"
     ]
    }
   ],
   "source": [
    "x_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n"
     ]
    }
   ],
   "source": [
    "x_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征选择\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "x_train = x_train[features]\n",
    "x_test = x_test[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拼接x_train和x_test两个数据集，以整个数据的均值或高频值填充缺失项\n",
    "data = x_train.append(x_test)\n",
    "assert data.shape[0] == x_train.shape[0] + x_test.shape[0]\n",
    "data.reset_index()\n",
    "\n",
    "# Age\n",
    "import math\n",
    "x_train.Age = pd.Series(map(math.floor,x_train.Age.fillna(data.Age.mean())))\n",
    "x_test.Age = pd.Series(map(math.floor,x_test.Age.fillna(data.Age.mean())))\n",
    "\n",
    "# Embarked\n",
    "x_train.Embarked = x_train.Embarked.fillna(data.Embarked.value_counts().index[0])\n",
    "\n",
    "# Fare\n",
    "x_test.Fare = x_test.Fare.fillna(data.Fare.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将没有数值大小关系的特征进行独热编码\n",
    "x_train = pd.get_dummies(x_train, columns=['Pclass','Sex','Embarked'])\n",
    "x_test = pd.get_dummies(x_test, columns=['Pclass','Sex','Embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scale(train_data, test_data, features=None, mode='Standar'):\n",
    "    '''\n",
    "    train_data: 训练集\n",
    "    \n",
    "    test_data: 测试集\n",
    "    \n",
    "    features: 将要进行归一化处理的特征列表。默认值为None, 对所有特征进行归一化处理。\n",
    "              \n",
    "    mode: 'Standar'为StandardScaler; 'MinMax'为MinMaxScaler.\n",
    "    '''\n",
    "    # Scale mode\n",
    "    if mode == 'Standar':\n",
    "        Scaler = StandardScaler()\n",
    "    elif mode == 'MinMax':\n",
    "        Scaler = MinMaxScaler()\n",
    "    else : \n",
    "        print('the Scale mode selected has not suppoted ')\n",
    "        return None\n",
    "    \n",
    "    # copy of data\n",
    "    s_train = train_data.copy(deep=True)\n",
    "    s_test = test_data.copy(deep=True)\n",
    "    \n",
    "    # Scaling\n",
    "    if features == None:\n",
    "        s_train = Scaler.fit_transform(s_train)\n",
    "        s_test = Scaler.transform(s_test)\n",
    "        return s_train, s_test\n",
    "    \n",
    "    elif type(features) == list:\n",
    "        s_train.loc[:,features] = Scaler.fit_transform(s_train.loc[:,features])\n",
    "        s_test.loc[:,features] = Scaler.transform(s_test.loc[:,features])\n",
    "        return np.array(s_train), np.array(s_test)\n",
    "    \n",
    "    else:\n",
    "        print(' ''List'' type of features requested!')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandarScaled\n",
    "ss_train, ss_test = Scale(x_train, x_test, features=['Age'], mode='Standar')\n",
    "\n",
    "# MinmaxScaled\n",
    "mn_train, mn_test = Scale(x_train, x_test, features=['Age'], mode='MinMax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_csv(x_test_id, prediction, name):\n",
    "    '''保存预测结果'''\n",
    "    return pd.DataFrame({'PassengerId': x_test_id, 'Survived': prediction}).set_index('PassengerId').to_csv(name + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "\n",
    "# 模型定义\n",
    "dtc = DTC()\n",
    "\n",
    "# 模型训练\n",
    "dtc.fit(ss_train, y_train)\n",
    "\n",
    "# 结果预测\n",
    "ss_dtc_predictions =  dtc.predict(ss_test)\n",
    "\n",
    "# 保存预测结果\n",
    "export_csv(test_id, ss_dtc_predictions, name='ss_dtc_predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逻辑回归\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 模型定义\n",
    "LR = LogisticRegression()\n",
    "\n",
    "# 模型训练\n",
    "LR.fit(ss_train, y_train)\n",
    "\n",
    "# 结果预测\n",
    "ss_lr_predictions = LR.predict(ss_test)\n",
    "\n",
    "# 保存预测结果\n",
    "export_csv(test_id, ss_lr_predictions, name='ss_lr_predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TPOT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc52e9a6de14fb899b424d5d6988f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=300.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.8328102441780179\n",
      "Generation 2 - Current best internal CV score: 0.8328102441780179\n",
      "Generation 3 - Current best internal CV score: 0.8350134957002073\n",
      "Generation 4 - Current best internal CV score: 0.8350134957002073\n",
      "Generation 5 - Current best internal CV score: 0.8350134957002073\n",
      "Best pipeline: RandomForestClassifier(XGBClassifier(input_matrix, learning_rate=0.1, max_depth=5, min_child_weight=11, n_estimators=100, nthread=1, subsample=0.9000000000000001), bootstrap=False, criterion=gini, max_features=0.1, min_samples_leaf=5, min_samples_split=2, n_estimators=100)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "# TPOTClassifier定义\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)\n",
    "\n",
    "# 模型训练\n",
    "tpot.fit(ss_train, y_train)\n",
    "\n",
    "# 预测\n",
    "ss_tpot_prediction = tpot.predict(ss_test)\n",
    "\n",
    "# 保存预测结果\n",
    "export_csv(test_id, ss_tpot_prediction, name='ss_tpot_prediction')\n",
    "\n",
    "# 导出模型文件\n",
    "tpot.export('ss_pipeline.py')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
