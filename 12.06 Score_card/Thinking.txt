A: 逻辑回归的假设条件是怎样的？
Q: 逻辑回归解决的是二分类问题，即预测0/1，因此其服从伯努利分布

A: 逻辑回归的损失函数是怎样的？
Q: 对于二分类，常用的损失函数是logloss交叉熵，对于不同的预测结果反馈不同的损失，一般可以运用梯度下降来优化参数。

A: 逻辑回归如何进行分类？
Q: 逻辑回归是线性回归与Sigmoid的组合，其将线性回归的最终结果通过Sigmoid映射到0~1的取值区间，通过设定阈值(一般为0.5)，最终的到0或1的结果。
逻辑回归虽然简单好用，但是其表达能力不强，只能处理线性可分的数据，且无法进行特征交叉，导致准确率不高。当然，也可以引入人工特征组合，但这就需要花费大量时间做特征工程，需要对业务有足够的理解。
而Facebook提出的GBDT+LR就很好地解决了这一问题，其利用树模型的特征组合能力，为我们筛选出重要的特征并进行组合，再将组合后的特征输入LR，这大大增强了模型的表达能力，为之后的改进提供了新的思路。

A: 为什么在训练中需要将高度相关的特征去掉？
Q: 由于高相关度的特征之间可以通过线性变换来相互表示，即多重共线性。其使得线性回归变得不确定或不精确、方差变大标准误差增大；削弱特征变量的重要性；甚至可能使得估计的回归系数符号相反，得出错误的结论
