Q1: 什么是反向传播中的链式法则
A1: 反向传播中的链式法则就是对多层神经网络组成的复合函数求导，反向传播的对象是导数，也成为梯度。整个的模型参数更新的方向是损失函数
      对该参数求偏导或导数的负方向。配合学习率，指导模型沿着梯度下降的方向，向损失函数极小值拟合。

Q2: 请列举几种常见的激活函数，激活函数有什么作用
A2:  激活函数的作用是增加模型的非线性，如果没有激活函数，模型做再多线性操作的叠加，最终得到的模型还是线性的，因此导致模型的表达能力不强。 	
     
     常用的激活函数有：
         sigmoid，取值范围(0，1)，适合做二分类，而多分类可选用softmax。由于sigmoid的导数在(-5，5)以外的区域接近于0，
                  所以当多个sigmoid叠加时容易产生梯度消失；
                  
         tanh，取值范围(-1，1)，奇函数，均值为0，和sigmoid一样，在输入很大或者很小时，梯度很小，容易出现梯度消失问题；
         
         relu，小于零的部分为0，大于零的部分为输入本身，形式较为简单，又存在非线性，因此拟合较快。但直接将小于零的输入
               直接置零，会存在信息丢失的问题，因此也有很多基于relu的变形 elu, selu, leaky relu等等。

Q3: 利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？
A3: loss不变可能有一下问题：1. 计算了loss，也计算了梯度，但忘记更新权重，导致模型并没有更新
    loss不降可能有一下问题：1. 问题：学习率太大，导致训练过程中loss减小后增大(无法收敛到极小值后，又因步长太大而远离极小值)。
                             解决方法：降低学习率，或者按阶段设置学习率，前期先用大的学习率加速逼近极小值，后期减小学习率，找到极小值。
                          2. 问题：如果是已经训练了很多个epoch，loss停止下降，这就是模型已经拟合了，如果再训练下去可能val_loss会上升，产生过拟合
                             解决办法：停止训练，如果此时模型的性能并不能达到实际的要求，可以选择调试超参数(学习率的变化策略，网络层数，优化器等等)，
                                      或者是数据预处理的问题，还可能是模型自身表达能力的问题。
