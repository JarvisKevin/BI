Q1: CNN中的参数共享指的是什么？
A1: CNN中的参数共享指的是每个卷积核以滑动窗口的形式扫描图片时，对于图片上任意位置的参数都是一样的，相当于一个卷积核只学习一种类型的特征，因此，每个CNN层都含有多个卷积核，以提取多种不同类型的特征。

Q2: 为什么会用到Batch Normalization(BN)?
A2: 协变量偏移：在机器学习中，一般会假设模型的输入数据的分布是稳定的，如果模型的输入数据的 分布发生变化，则称为协变量偏移。
    对于多个子模块堆叠而成的机器学习系统，我们也希望各个子模块的输入分布是稳定的，若不稳定，则称为内部协变量偏移。对于深度神经网络，
    其每一层的参数都会随着训练过程更新，而第i层输入的分布也会随着之前层的参数更新而发生改变。网络越深，内部协变量偏移发生的概率越大，
    这将会带来很多问题：1.每一层的神经元需要不断地适应不同分布的输入数据，这将影响学习效率，是学习过程变得不稳定。
                       2. 为了降低内部协变量偏移的影响，一般会采用小学习率进行训练。而这会降低模型的收敛速度。

    因此，Batch Normalization就是确保即使在参数发生变化的情况下，网络中的每一层的输入/输出数据的分布也不会产生较大变化，从而避免内部协变量偏移现象。

    批归一化可以看作是带参数的标准化，其加入了可学习的平移参数和缩放参数。

    若没有这两个参数，BN则退化成标准化，网络中每一层的输出分布均为均值0，方差1，这样不利于模型的学习。而且，对于Sigmoid，Tanh等激活函数来说，
    均值0，方差1的数据正好落在线性区，没能体现激活函数的非线性特性，这会极大地削弱模型的表达能力。添加了这两个参数后，就相当于每一层都有一个量
    身定制的分布，保留了每个神经元的学习结果，且让数据更好地落在激活函数的非线性区。

Q3: 使用dropout可以解决什么问题？
A3: dropout就是随机丢弃一些神经元(不和其他神经元进行连接)，这是为了防止模型过拟合而产生的策略(有点自我增加噪音，或bagging有放回地抽取神经元，
    减少偏差，增加模型的泛化能力的味道)，dropout是针对于全连接来说的，对于CNN，有dropblock策略，两者思想一致。
